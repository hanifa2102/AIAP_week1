{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Greetings traveller! \n",
    "Welcome to Week 1 of the AIAP coursework. In the next 6 weeks, you will be *travelling* through the machine learning *space*, where we will be looking at various different machine learning problems and how to solve them. We will be taking a quick sweep through the many areas of analytics and machine learning, including learning about learning from data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Course Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This course is designed to be different from the courses you have done previously. We acknowledge that there are lots of great free resources online, and are not trying to create yet another tutorial. Rather, we attempt to provide something that complements them, mirroring real-life problem solving. In general, we have the following goals out of the programme:\n",
    "- __Finger-dipping exposure into ML__: we concede that six weeks are insufficient to fully understand ML, and we do not aim to do that. Rather, we would like to provide sufficient breadth in terms of practical, useful knowledge in the area.\n",
    "- __Confidence to go further yourself__: the ML space is vast and expanding every day, no practitioner is ever sufficently trained to tackle any problem. Rather, good data scientists hone a sharp ability to learn new techniques to solve novel problems. We wish to build your confidence to go into the unknown, so that you can rely on yourself for learning beyond textbook knowledge.\n",
    "- __Programming competency__: while we are not training software engineers, writing good, clean code is crucial to the success of any project that requires programming. We will provide guidance on how to write reproducible, human-centric code for data science that will pay dividends for a project in the long term.\n",
    "- __Employability__: ultimately, we aim to help you get hired in the data science space, and we have crafted our notebooks to act like mock technical assessments in a safe space. The six week programme will equip you with critical soft skills for data science as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Prerequisite Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is certainly not a course for beginners. Considering that this will be a full-time, six-week programme, we have designed the course to be challenging in every aspect. Realistically, we would not expect anyone to be comfortable handling the course. Although the programme will be challenging, we wish to cultivate a forgiving, learning-based culture where necessary failure is celebrate encouraged and celebrated. Nevertheless, the following prerequisites will help you do well in this course:\n",
    "- __Python, or general programming skills__: an ability to execute basic tasks beyond hello world in Python, or simply being comfortable with computer languages in general\n",
    "- __Numerical programming__: an ability to execute mathematical scripts through a programming language like Python (it will be relatively easy to transition from Matlab, R, SAS or Julia)\n",
    "- __Statistical fundamentals__: you should know how basic tools like linear and logistic regression work, and have a mathematical appreciation for it\n",
    "- __Linear algebra and calculus__: basic mathematical knowledge will help you appreciate the algorithms and learn to use them better\n",
    "- __A positive learning attitude__ most importantly, because realistically, no one will have all of the above, so we will all need to adapt and learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Learning Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At one or more points in time while attempting this notebook, you may find the following resources to be useful:\n",
    "- Intro to Python Programming: [Python at PluralSight](https://app.pluralsight.com/paths/skills/python)*\n",
    "- Numerical computing in Python: [Python for Data Science, 2nd Ed. by Wes McKinney](http://wesmckinney.com/pages/book.html)\n",
    "- Random Forests in depth and from scratch: [fast.ai Machine Learning Course](https://course.fast.ai/ml)\n",
    "- Organising your notebook: [initial steps toward reproducible research by Karl Broman](https://kbroman.org/steps2rr/)\n",
    "- Finding specific methods in Pandas: [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/)\n",
    "- All the answer keys you'll ever need: [Kaggle's Titanic Kernel](https://www.kaggle.com/c/titanic/kernels)\n",
    "\n",
    "*You'll get free access to Pluralsight's Python track through your DataCamp subscription!\n",
    "\n",
    "These are just recommended resources - do tap on anything you find useful, or approach us for alternative recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Collaboration Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Collaboration is the best way to learn. In short, we should optimise learning - the general rule is to try everything yourself first, then discuss your method with your teammates. Do not directly copy their code, unless you are trying to learn a programming technique. Be smart and flexible - do your own thinking and write your own code.\n",
    "\n",
    "If you believe referring to someone's answer is the best way to learn, we recommend looking at the code, then walking away for a few minutes, and come back to write your own version of the code. There will be minimum policing, but we expect no plagurism or direct copying of code to occur.\n",
    "\n",
    "Please list your collaborators:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- John Ang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Time Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this notebook, there will two main areas of focus: modelling, and model implementation (numerical programming). Different people will have different strengths, and our advice is to play to your strengths, and collaborate to learn from people who can teach you something, but offer something in return. Unless you are really good with everything the notebook needs you to do, __you are not likely to finish the notebook by doing it alone__.\n",
    "\n",
    "For those who are very new to programming, it's okay to realise that you might not finish the notebook this week. If you are feel that you might not be ready to mix different technical fields together, considering spending more time building up your fundamentals instead - take your time if you need to. If you need more assistance, don't hesitate to speak to us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, let's get into week one of the course! __Good luck, have fun.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Initial Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our problem this week is the el classico: the Titanic dataset, a dataset probably done to death by fellow travellers of the machine learning space. There is a reason why this dataset is so popular - it demands for all the fundamentals required of statistical modelling, while staying light in terms of technical demands. With just 891 rows of data, the problem can be solved on any laptop. While your laptop would not face much stress this week, we would recommend you to consider your technical set-up, so that as heavier datasets come about (in the size of GBs), you will not be limited by them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, go grab the data. The data is available at https://www.kaggle.com/c/titanic/data, you will need to register an account to retrieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we are talking about downloading data, we should take this time to set up your folder. One such way (our recommended way) to do it as to create a project folder, then leave your notebooks in the root of that folder. Your code base, which we will be starting to build over time, should be in a `src` subfolder, while your data should be in a `data` folder, with a tree structure as shown:\n",
    "\n",
    "```\n",
    "aiap\n",
    " |- src\n",
    " |- data\n",
    " |   |- titanic.csv\n",
    " |   |- titanic_test.csv\n",
    " |- week1.ipynb\n",
    "```\n",
    "\n",
    "This is an opinionated format - we suggest this only for simplicity reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, we are ready to do some coding work. First, import the necessary libraries you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Import your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic_raw = pd.read_csv('data/train.csv')\n",
    "titanic_cp=titanic_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     if 'pdp' in str(toPrint):\n",
    "#         test_df=pd.concat([X_test,y_test],axis=1)\n",
    "#         fig, axes, summary_df = info_plots.target_plot(\n",
    "#         df=test_df, feature='Sex_male', feature_name='gender', target='Survived')\n",
    "        \n",
    "#         fig, axes, summary_df = info_plots.actual_plot(model=rfc, X=test_df[X.columns],feature='Sex_male', feature_name='gender')\n",
    "#         pdp_sex = pdp.pdp_isolate(\n",
    "#     model=rfc, dataset=test_df, model_features=X.columns, feature='Sex_male')\n",
    "#         fig, axes = pdp.pdp_plot(pdp_sex, 'Sex_male')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Initial Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train a stock random forest model (with no custom parameters and report the accuracy score). Do the __minimal__ cleaning required to let your model fit into the model. Using the `train_test_split` method, reserve some validation data for evaluation use.\n",
    "\n",
    "You may see an accuracy of approximately 75% - this does not mean anything substantially, but it lets us know that a no-value-add approach to modelling will already generate this accuracy for us. Hence, our goal is to improve upon this current score, and reach as high as possible. For now, 75% is good enough for us to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unneccesary columns which dont contribute to modelling now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all independant variables with NA values and Sex columns (dummy code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp.drop(['PassengerId','Name','Ticket'],axis='columns',inplace=True)\n",
    "titanic_cp.drop(['Age','Cabin'],axis='columns',inplace=True)\n",
    "titanic_cp.dropna(inplace=True) #Drop Embarked Rows with NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DONE__\n",
    "-  Converting to category doesnt help (Not sure if there is a similar to as.factor method in R)\n",
    "-  LabelEncoder method is used. Not sure for non-binary variables\n",
    "\n",
    "- __Answered Label Encoding just assigns numeric values to categorical values like Embarked (not ordinal).__\n",
    "- __Must do one hot encoding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder = preprocessing.LabelEncoder()\n",
    "# titanic_cp['Embarked_Label'] = labelencoder.fit_transform(titanic_cp['Embarked'])\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# onehotencoder = preprocessing.OneHotEncoder(categorical_features = [0])\n",
    "# x = onehotencoder.fit_transform(x).toarray()\n",
    "# titanic_cp['Sex_Male'] = titanic_cp['Sex'].map( {'male':1, 'female':0} )\n",
    "\n",
    "\n",
    "# one hot encoding for remainining multiclass columns\n",
    "titanic_cp['Sex_Male'] = (titanic_cp['Sex'] == 'male').astype('int')\n",
    "titanic_cp['Embarked_S'] = (titanic_cp['Embarked'] == 'S').astype('int')\n",
    "titanic_cp['Embarked_C'] = (titanic_cp['Embarked'] == 'C').astype('int')\n",
    "titanic_cp = titanic_cp.drop(['Sex', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results of the Base model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the large difference in the training/testing accuracy, you can conclude that there is significant overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.9372990353697749\n",
      "Testing accuracy is 0.7715355805243446\n",
      "Confusion Matrix\n",
      "[[142  25]\n",
      " [ 36  64]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       167\n",
      "           1       0.72      0.64      0.68       100\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       267\n",
      "   macro avg       0.76      0.75      0.75       267\n",
      "weighted avg       0.77      0.77      0.77       267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = titanic_cp.drop('Survived',axis='columns')\n",
    "y = titanic_cp['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)\n",
    "rfc = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred_training=rfc.predict(X_train)\n",
    "print('Training accuracy is '+ accuracy_score(y_train,rfc_pred_training).astype('str'))\n",
    "\n",
    "\n",
    "rfc_pred= rfc.predict(X_test)\n",
    "print('Testing accuracy is '+accuracy_score(y_test,rfc_pred).astype('str'))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,rfc_pred))\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,rfc_pred))\n",
    "\n",
    "\n",
    "# _ = axes['pdp_ax'].set_xticklabels(['Female', 'Male'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "test_df=pd.concat([X_test,y_test],axis=1)\n",
    "\n",
    "# cols=X_test.columns\n",
    "# for col in cols:\n",
    "#     pdp_sex = pdp.pdp_isolate(model=rfc, dataset=test_df, model_features=X_test.columns, feature=col)\n",
    "#     fig, axes = pdp.pdp_plot(pdp_sex, col)\n",
    "\n",
    "\n",
    "pdp_fare = pdp.pdp_isolate(\n",
    "    model=rfc, dataset=test_df, model_features=X_test.columns, feature='Fare')\n",
    "\n",
    "fig, axes = pdp.pdp_plot(pdp_fare, 'Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Exploring the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Overall Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Conduct some initial exploration of this data. This could be through dataset level plots, correlation charts and table describes, as well as by understanding what kind of information is available, or not available (i.e. missing). Write a paragraph on what you observe in the data. There is no correct answer, but do present useful and insightful information as much as possible.\n",
    "\n",
    "This part of the notebook should be helpful to someone who is trying to come into your project, but has no knowledge of the data. In complex, real-world problems, there may be multiple data sources, each with different structures of data. Making sense of data at this macro level may happen over several months in an iterative manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NA detection\n",
    "-  Age,Cabin and Embarked have columns with NA.\n",
    "-  For Age, we will try to impute the values for the missing rows while Embarked, it is just two missing rows, so we will drop those two rows.\n",
    "-  For cabins, there are only 204 obs with values, so skipping that column might be a good solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall observation\n",
    "-  More than 75% of passengers are under 38 years\n",
    "-  At least 75% have no parents/children\n",
    "-  At least 50% have siblings\n",
    "-  75% of fares are under the average fare with the max being almost $512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.describe()\n",
    "#Figure out how to remove those zeros|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp=titanic_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp.drop(columns=['PassengerId','Name','Cabin'],inplace=True)\n",
    "titanic_cp.dropna(inplace=True)\n",
    "titanic_cp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot observation\n",
    "- Pclass survival rates are higher in 1st class compared to 3rd class\n",
    "- Age doesnt seems a clear indicator\n",
    "- Having more siblings helped with survival\n",
    "- Having parents/children no clear correlation\n",
    "- Lower paying fare passengers had low survival rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "sns.pairplot(titanic_cp,palette='coolwarm',hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Individual Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now go deeper into individual variables. For each variable, provide plots, tables or descriptions that best capture the nuance of that column. There is no correct answer, but there is a gold standard.\n",
    "\n",
    "Samples of this can be found in Kaggle's kernels page. While we value pretty charts, we value insights much more. Where insightful information is found, please indicate them in your notebook for your reader.\n",
    "\n",
    "You may wish to strategically go deeper into variables you find more interesting. There is no need to scrutinize every variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp=titanic_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### PassengerId\n",
    "-  Running order index, wont be useful for learning any patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Survival rate for the Titanic crash is 38%. \n",
    "-  But the dependant variable is quite balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Survived',data=titanic_cp,palette='Greens_d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The survival rate seems to be higher in the 1st class compared to the 3rd class and also most passengers are from 3rd class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Pclass\",data=titanic_cp,hue=\"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Fares in the 1st class are higher than in 2nd and 3rd class in general. In the 1st class, passengers who survived paid a higher fare in general, though there is no such correlation in the 2nd and 3rd class.\n",
    "-  Above the $100 mark, survivability looks better for passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Pclass\", y=\"Fare\", data=titanic_raw,hue='Survived')\n",
    "sns.lmplot(x='Fare',y='Survived',data=titanic_raw,logistic=True,y_jitter=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Names are unique. At this point, not sure on how to utilize for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic_raw['Name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The survival rate seems to be quite high for women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Sex\",data=titanic_cp,hue=\"Survived\",palette=\"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age seems to correlated to the PClass with, the lower classes having a younger age grouping. We could use this info to impute the age later onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Pclass\", y=\"Age\",hue=\"Survived\", data=titanic_raw,palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SibSp / Parch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Passengers who have 1-2 siblings were more likely to survive, however having multiple siblings meant lower survival rate.\n",
    "-  Passengers with 1-3 parents/children are more likely to survive\n",
    "-  Also some correlation between these two variables. Small sized family seem to have higher survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"SibSp\",y=\"Survived\",data=titanic_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"Parch\",y=\"Survived\",data=titanic_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"SibSp\",y=\"Parch\",data=titanic_raw,hue='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic_raw.groupby(by=[\"Parch\",\"SibSp\",\"Survived\"])[\"PassengerId\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Most passengers about 547 have 1 ticket, but the rest of them seem to share tickets with 94 couples etc. \n",
    "-  Passengers sharing 2 or 3 tickets have a higher survival rate than the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=titanic_cp[['PassengerId','Ticket']].groupby('Ticket')['PassengerId'].count()\n",
    "df1=t1.to_frame().reset_index()\n",
    "df1.columns = ['Ticket', 'PassengersPerTicket']\n",
    "df1['PassengersPerTicket'].value_counts()\n",
    "#pd.merge(left,right,how='inner',on='key')\n",
    "df2=pd.merge(titanic_cp,df1,how='inner',on='Ticket')\n",
    "print(df2[['PassengersPerTicket','Survived']].groupby('PassengersPerTicket').mean().sort_values(by='PassengersPerTicket', ascending=True))\n",
    "\n",
    "sns.countplot(x=\"PassengersPerTicket\",hue='Survived',data=df2,palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 687 NA values for this dataframe thus, this column would be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic_raw[['Cabin']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"Embarked\",y=\"Survived\",data=titanic_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fares paid in embarkation ports are in this order C>S>Q. Amongst those who embarked from C and paid a higher fare, survival was higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Embarked\", y=\"Fare\", data=titanic_raw,hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Overall Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From your own exploration of the data, provide a few paragraphs in summary of the dataset. At this point, it may be helpful to provide a narrative which can reconstruct the situation aboard the titanic as it was sinking. This is also an opportunity to direct your attention towards areas where you feel information is raw and can be improved in your next section, through feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Overall, it appears that some variables are more significant than others, but we would imagine that females and children, especially those who embarked at 'C' and are affluent, hence being able to pay for expensive tickets, will survive, while young men whom travelled alone with no kids or parents are most likely to have perished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passengers travelling in 1st class (who paid higher fares) and females had significantly higher survival rates. The average age was typically lower in 3rd class compared to the 1 and 2nd class.Having 2-3 siblings or 2-3 parents gives a higher survival rate, this could be due to helping one another, however it doesnt scale up (when the relationship is too large, the impetus to save all results in disaster. Those with shared tickets typically paid higher fares and higher survival rates than those with single ticket per passenger. Those who embarked from C paid higher fares and also more likely to survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Model Interpretation and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we will begin by learning to appreciate the model interpretation methods related to decision trees and random forests. Please do some of your own research about these approaches. Then, we will move on to do some feature engineering - hopefully this will give us some information gain with respect to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Feature importance\n",
    "Plot a graph/table of feature importance of variables. Is there anything to be expected out of the data? Is there anything unexpected? Compare these findings with your teammate - are there any major differences in these plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            importance\n",
      "Fare          0.407660\n",
      "Sex_Male      0.303712\n",
      "Pclass        0.105591\n",
      "Parch         0.070709\n",
      "SibSp         0.062816\n",
      "Embarked_S    0.024862\n",
      "Embarked_C    0.024650\n"
     ]
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(rfc.feature_importances_,index = X_train.columns,\n",
    "                        columns=['importance']).sort_values('importance',ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Partial Dependence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another useful interpretation plot is partial dependence. `sklearn` might not have a workable library out of the box, so one option would be to try `pdpbox`.<br/>\n",
    "[Introducing PdpBox](https://towardsdatascience.com/introducing-pdpbox-2aa820afd312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__\n",
    "- pdpbox error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "#test Actual Data (works)\n",
    "test_df=pd.concat([X_test,y_test],axis=1)\n",
    "fig, axes, summary_df = info_plots.target_plot(\n",
    "df=test_df, feature='Sex_Male', feature_name='gender', target='Survived')\n",
    "\n",
    "#test Predicated Data (error)\n",
    "# fig, axes, summary_df = info_plots.actual_plot(\n",
    "#     model=rfc_1, X=X_test_1, feature='Sex_male', feature_name='gender')\n",
    "\n",
    "######  model=rfc_1, X=titanic_data[titanic_features], feature='Sex_male', feature_name='gender'\n",
    "\n",
    "#Model feature predictiveness (error)\n",
    "#fig, axes, summary_df = info_plots.actual_plot(model=rfc, X=test_df[X.columns],feature='Sex_Male', feature_name='gender')\n",
    "\n",
    "# pdp_sex = pdp.pdp_isolate(model=rfc_1, dataset=test_df_1, model_features=X_1.columns, feature='Sex_male')\n",
    "# fig, axes = pdp.pdp_plot(pdp_sex, 'Sex_male')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, you should first briefly explain your thought process - what is good, what is lacking, and what are the potential areas of information the model has yet to exploit. Following which, do some feature engineering. After every engineered feature, re-run your model and observe if there is an improvement in scores.\n",
    "\n",
    "Running your feature importance again at different points in time can help to validate if your variables are truly important, or are they simply collinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Dropping Columns\n",
    "- PassengerId as it is Running index\n",
    "- Cabin has too many NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp=titanic_raw.copy()\n",
    "titanic_test=pd.read_csv('data/test.csv')\n",
    "titanic_datasets=[titanic_cp,titanic_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_cp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using KNN to impute Age\n",
    "[Article_on_KNN](https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__\n",
    "- TypeError: 'module' object is not callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import src.knn_impute as knn_impute\n",
    "# from src.decision_tree import knn_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_impute(target=titanic_cp['Age'], attributes=titanic_cp.drop(['Age', 'Survived','Name'], 1),\n",
    "#                                     aggregation_method=\"median\", k_neighbors=10, numeric_distance='euclidean',categorical_distance='hamming', missing_neighbors_threshold=0.8)\n",
    "\n",
    "#Wasnt able to make Knn work, using Mean/Mode/etc for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Age inputation, we can use the Pclass, as we had earlier established that Pclass in correlated to Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in titanic_datasets:\n",
    "    df[\"Age\"].fillna(df.groupby(\"Pclass\")[\"Age\"].transform(\"mean\"),inplace=True)\n",
    "    df['Embarked'].fillna(value=df['Embarked'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in titanic_datasets:\n",
    "    df['Family_size']=df['SibSp']+df['Parch']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in titanic_datasets:\n",
    "#     print(df.columns)\n",
    "#     df1=df['Ticket'].value_counts()\n",
    "#     df2=df1.to_frame().reset_index().rename(columns={'index':'Ticket','Ticket':'PassegnersPerTicket'})\n",
    "#     df=pd.merge(df,df2,how='inner',on='Ticket')\n",
    "#     print(df.columns)\n",
    "# For loop doesnt work\n",
    "\n",
    "\n",
    "#Refactor this code. Breaks something\n",
    "\n",
    "# df1=titanic_cp['Ticket'].value_counts()\n",
    "# df2=df1.to_frame().reset_index().rename(columns={'index':'Ticket','Ticket':'PassegnersPerTicket'})\n",
    "# titanic_cp=pd.merge(titanic_cp,df2,how='inner',on='Ticket')\n",
    "\n",
    "\n",
    "# df1=titanic_test['Ticket'].value_counts()\n",
    "# df2=df1.to_frame().reset_index().rename(columns={'index':'Ticket','Ticket':'PassegnersPerTicket'})\n",
    "# titanic_test=pd.merge(titanic_test,df2,how='inner',on='Ticket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read \n",
    "- [Label Encoding](https://www.kaggle.com/getting-started/27270)\n",
    "\n",
    "__TOCHECK__\n",
    "- One hot encoding not neccesary for tree models (is it true)\n",
    "- Just use label encoding for columns with more than 2 unique values but if only 2, then can do manual encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder = preprocessing.LabelEncoder()\n",
    "for df in titanic_datasets:\n",
    "    df['Sex_Male'] = (df['Sex'] == 'male').astype('int')\n",
    "    df['Embarked_S'] = (df['Embarked'] == 'S').astype('int')\n",
    "    df['Embarked_C'] = (df['Embarked'] == 'C').astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in titanic_datasets:\n",
    "    df.drop(columns=['Cabin','PassengerId','Name','Ticket'],inplace=True)\n",
    "    df.drop(columns=['Sex','Embarked'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test.fillna(titanic_test['Fare'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Fare           891 non-null float64\n",
      "Family_size    891 non-null int64\n",
      "Sex_Male       891 non-null int32\n",
      "Embarked_S     891 non-null int32\n",
      "Embarked_C     891 non-null int32\n",
      "dtypes: float64(2), int32(3), int64(5)\n",
      "memory usage: 59.2 KB\n"
     ]
    }
   ],
   "source": [
    "titanic_cp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. Model Re-Training and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRandomForest(df):\n",
    "\tX = df.drop('Survived',axis='columns')\n",
    "\ty = df['Survived']\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)\n",
    "\trfc = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "\n",
    "\trfc.fit(X_train, y_train)\n",
    "\trfc_pred= rfc.predict(X_test)\n",
    "\trfc_pred_training=rfc.predict(X_train)\n",
    "\n",
    "\tprint('Testing accuracy is '+accuracy_score(y_test,rfc_pred).astype('str'))\n",
    "\tprint('Traing accuracy is '+ accuracy_score(y_train,rfc_pred_training).astype('str'))\n",
    "\n",
    "\tprint(\"Confusion Matrix\")\n",
    "\tprint(confusion_matrix(y_test,rfc_pred))\n",
    "\tprint(\"Classification Report\")\n",
    "\tprint(classification_report(y_test,rfc_pred))\n",
    "\n",
    "\tprint(\"Feature Importance\")\n",
    "\tfeature_importances = pd.DataFrame(rfc.feature_importances_,index = X_train.columns,\n",
    "\t\t\t\t\t\t\t   columns=['importance']).sort_values('importance',ascending=False)\n",
    "\tprint(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Re-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When you are confident of your variables, re-run your model with all your variables again, and observe your feature importance. At times having extra variables may even deprove scores. You may also wish to remove features that show insignificant partial dependence. \n",
    "\n",
    "How much accuracy did these engineered features give? How important were these features? At this point in time, you may wish to talk to your peers and identify features they came up with (original ones, not those taken from the internet). This is a stage where brainstorming and contextual knowledge is extremely helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is 0.7835820895522388\n",
      "Traing accuracy is 0.9807383627608347\n",
      "Confusion Matrix\n",
      "[[130  27]\n",
      " [ 31  80]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       157\n",
      "           1       0.75      0.72      0.73       111\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       268\n",
      "   macro avg       0.78      0.77      0.78       268\n",
      "weighted avg       0.78      0.78      0.78       268\n",
      "\n",
      "Feature Importance\n",
      "             importance\n",
      "Fare           0.261645\n",
      "Sex_Male       0.255776\n",
      "Age            0.249187\n",
      "Pclass         0.081430\n",
      "Family_size    0.054576\n",
      "SibSp          0.030925\n",
      "Parch          0.025077\n",
      "Embarked_S     0.022111\n",
      "Embarked_C     0.019274\n"
     ]
    }
   ],
   "source": [
    "runRandomForest(titanic_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Tuning - Good for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we should do some model tuning. We previously ran a \"default\" model, with no customization inside our RandomForestClassifier model. However, if we were to look at the parameters, we'll see that there are many you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Evidently, there are many variables worth checking out. For now, some of the most salient ones are `max_depth`, `max_leaf_nodes`, `max_features` and `n_estimators`. These are in general, all parameters we tweak to decrease overfitting. Try tuning these parameters, plotting a graph of model accuracy against parameter variation for each variable.\n",
    "\n",
    "Other useful parameters are `oob_score`, which serves as a validation set of unsampled data points during the bootstrap, and `n_jobs`, which parallelises the process. We recommend you set `oob_score` to `True` (and use the oob_score as a metric), and `n_jobs` to `-1` to speed up your training process.\n",
    "<br /><br />\n",
    "<font color=red>This is not a prerequisite per se, but at this point, you should try to understand the bootstrapping concept. After all, this single concept gave rise to random forests and many other statistical methods we know today!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = titanic_cp.drop('Survived',axis='columns')\n",
    "y = titanic_cp['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, \n",
    "                               random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 2000,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 100,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DONE__\n",
    "- With iteration, these best params values keeps changing\n",
    "- __Set a random state to RFC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 729 out of 729 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'bootstrap': [False], 'max_depth': [80, 90, 100], 'max_features': [2, 3, 4], 'min_samples_leaf': [3, 4, 5], 'min_samples_split': [9, 10, 11], 'n_estimators': [500, 1000, 1600]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [80,90,100],\n",
    "    'max_features': [2, 3, 4],\n",
    "    'min_samples_leaf': [3,4,5],\n",
    "    'min_samples_split': [9,10,11],\n",
    "    'n_estimators': [500,1000,1600]\n",
    "}\n",
    "# Create a based model\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 80,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 11,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is 0.7985074626865671\n"
     ]
    }
   ],
   "source": [
    "best_grid_model=grid_search.best_estimator_\n",
    "\n",
    "best_grid_pred=best_grid_model.predict(X_test)\n",
    "print('Testing accuracy is '+accuracy_score(y_test,best_grid_pred).astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Subsampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `fastai` library has a very cool method called `set_rf_samples`, which sets the number of subsamples we use in each tree we initalize. For more information, you may refer [here on stackoverflow](https://stackoverflow.com/questions/44955555/how-can-i-set-sub-sample-size-in-random-forest-classifier-in-scikit-learn-espec). You might wish to play with this variable, as it can give you some improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#SKIP\n",
    "titanic_cp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cross Validation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we think about what we do with validation, we're actually taking a portion (20%) of our data out of our training set for validation purposes. This means that we are sacrificing training data (and hence predictive power) to create a less overfitted, more generalised model. There is a trade-off for our model: we remove overfitting (variance) by sacrificing predictive power (increasing bias). This is known as the bias variance trade-off, which we will go into more detail next week.\n",
    "\n",
    "We will go into details next week, but in short, this can be avoided using cross validation. If you have done this before, you may use cross validation to improve the model here. Report your accuracy.\n",
    "\n",
    "Otherwise, if we know all the validation scores for all our models, simply pick the best model in terms of validation score. Report your accuracy. \n",
    "\n",
    "Put back all our data into one big training set, and re-train the model using this training set. You can now make a prediction on your test set, and submit your result to Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.8664421997755332\n"
     ]
    }
   ],
   "source": [
    "#Use the whole dataset\n",
    "\n",
    "best_grid_model=grid_search.best_estimator_\n",
    "\n",
    "best_grid_pred=best_grid_model.predict(X)\n",
    "print('Training accuracy is '+accuracy_score(y,best_grid_pred).astype('str'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=best_grid_model.predict(titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred,columns=['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test=pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pd.concat([df,titanic_test],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  PassengerId  Pclass  \\\n",
       "0         0          892       3   \n",
       "1         0          893       3   \n",
       "2         0          894       2   \n",
       "3         0          895       3   \n",
       "4         1          896       3   \n",
       "\n",
       "                                           Name     Sex   Age  SibSp  Parch  \\\n",
       "0                              Kelly, Mr. James    male  34.5      0      0   \n",
       "1              Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
       "2                     Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
       "3                              Wirz, Mr. Albert    male  27.0      0      0   \n",
       "4  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
       "\n",
       "    Ticket     Fare Cabin Embarked  \n",
       "0   330911   7.8292   NaN        Q  \n",
       "1   363272   7.0000   NaN        S  \n",
       "2   240276   9.6875   NaN        Q  \n",
       "3   315154   8.6625   NaN        S  \n",
       "4  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is your Kaggle leaderboard performance? Please provide your Kaggle username as well. We hope you have had a prediction accuracy of at least 78%, but it's okay if you don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions[['PassengerId', 'Survived']].to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Accuracy:  76%\n",
    "#### Kaggle name:  Hanifa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not all models are perfect, especially not in the constraint of time. Do some research on the models that do better than you, and list out the areas that you can improve on in the long run. Prioritise these improvements and spell out how you can implement them if they are non-trivial to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Ensembling/stacking instead of using a single RF model\n",
    "- More feature engineering \n",
    "    - On Unused cols like Name, Cabin\n",
    "    - Bucketing continuous variables like Fare,Age\n",
    "- To fix all knnimpute/pdpbox library calls\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5. Building your Random Forest from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Congratulations! You have completed the tutorial on random forests... not!\n",
    "\n",
    "Apart from modelling, each week, you will also be expected to implement the models we are using. After all, the best way to learn is to implement from scratch. AI Apprentices are not only expected to model, but also do the necessary engineering for real life problems, and many such problems require custom code. For example, we may want to use subsampling to improve our model performance, but edge-cutting methods would not yet be available in common libraries. When this happens, you will have to address these problems yourself.\n",
    "\n",
    "Numerical programming might be new to some, if you did not come from the R/Matlab side of things. To get yourself up to speed with numerical programming in Python, we highly recommend Wes McKinney's [Python for Data Analysis, 2nd Ed.](https://www.safaribooksonline.com/library/view/python-for-data/9781491957653/)\n",
    "\n",
    "__Note__: In this guided implementation, we made 2 decisions, firstly to use a Python `class`, i.e. object oriented programming (arguably so at least), and secondly to use the `numpy` library. Neither of these decisions are compulsory - if you have prior experience in another style, or using alternative libraries, feel free to do so, and modify the script to allow your code to run. However, if you have no prior experience, we suggest sticking to this format - we will follow `scikit-learn`'s format, which we believe is increasingly an industry standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A random forest, as the the name suggests, is made up of many decision trees, each with levels of variation and randomness. Before looking at random forests, we will look at understanding what decision trees do.\n",
    "\n",
    "Decision trees, more specifically Classification and Regression Trees (CARTs), are an algorithm/data structure that learns to split data out based on rules it learns. There are many resources out there to get a good understanding of what CARTs are, which you may wish to reference while accomplishing the tasks here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Gini Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you remember from `get_params`, there exists a parameter `criterion: 'gini'`. This means that the tree is using Gini as a criterion to decide how to separate the data.\n",
    "\n",
    "Hence, we will first learn how to use the Gini impurity score. The Gini impurity score of a node n is given as:  \n",
    "\n",
    "<center>$i(n) = 1 - p^2_0 - p^2_1$,  </center>  \n",
    "\n",
    "Where $p_1$ refers to the proportion of 1's in that node, and $p_0$ refers to the proportion of 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from src.decision_tree import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the above line of code to work, you will have to do the following if you haven't done so:\n",
    "1. Create a folder called src at the directory of your current notebook\n",
    "2. Create a __init__.py empty file in the src folder -see http://mikegrouchy.com/blog/2012/05/be-pythonic-__init__py.html\n",
    "3. Create a file, `decision_tree.py`. You can consider the terminal script `touch decision_tree.py`\n",
    "4. create a class `DecisionTree` inside `deicison_tree.py`\n",
    "\n",
    "You may realise that for this part of the coursework, we are not writing code directly into Jupyter notebooks, but inside the /src/ folder as `.py` files. We are maintaining a code base, outside of the Jupyter notebook. We do this for two reasons - 1) because this code is highly reusable in future sessions, beyond the scope of one notebook. 2) because such code bases are collaboration-friendly, as Git and Jupyter notebooks do not play well with each other, but python files do. In the future, non-exploratory code will be written in teams, so scripts would be a more collaboration friendly format. The `src/` folder structure is a very basic and light introduction to this, but in short, each project should have a different folder structure to cater to its needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def approx_eq(a, b, num_sig=5):\n",
    "    return round(a, num_sig) == round(b, num_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "approx_eq(DecisionTree().gini([1, 0, 0, 0, 0], [1, 1, 1, 1, 0]), .32)\n",
    "# for the above line of code to work,\n",
    "# 1. create a method gini that takes in 2 arrays and computes the node's gini impurity\n",
    "# 2. implement the method as per the mathematical formula given\n",
    "# 3. if you would like to turn this into a private method, make the necessary adjustments\n",
    "# -> DecisionTree()._DecisionTree__gini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Following sklearn's `fit` and `predict`/`score` approach to programming, we will be implementing the fit and predict methods. First, we will attempt to implement a fit method.\n",
    "\n",
    "The fit method will take in 2 numpy matrices: a m\\*n train array with m training examples of n features, and a m\\*1 array of labels.\n",
    "\n",
    "There are tons of resources available to describe the workings of a CART. We would encourage you to find a source that best suits your needs, but we have picked out two points which other resources may miss at the implementation stage. Feel free to find more resources to expand on these areas:\n",
    "\n",
    "1. The CART is a recursive tree structure. Every node of the tree can be seen as a decision tree node. When it splits, its left and right branches and its child nodes. When fitting a tree, you should recursively fit the nodes of the tree, in a way that the fitting can be used to predict in the future.\n",
    "\n",
    "2. In finding the best condition to split the variables, it is alright to iterate through every single unique value of every variable, and determine the best condition through the iterations. The best condition can be defined as the one that provides the most __information gain__, which is defined as the greatest loss in Gini impurity.\n",
    "\n",
    "If this is your first time doing object oriented programming in Python, we would high recommend you expose yourself to some Python resources first, or read the Python documentation. __If you need more help with these methods, ask a peer with programming experience, or you can seek help from the team.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read a new csv and remove complicated columns\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "X_cols = titanic.columns\n",
    "X_cols = X_cols.drop('Age')\n",
    "X_cols = X_cols.drop('Cabin')\n",
    "X_cols = X_cols.drop('Name')\n",
    "X_cols = X_cols.drop('Ticket')\n",
    "titanic = titanic[X_cols]\n",
    "\n",
    "# one hot encoding for remainining multiclass columns\n",
    "titanic['Sex_m'] = (titanic['Sex'] == 'male').astype('int')\n",
    "titanic['Embarked_S'] = (titanic['Embarked'] == 'S').astype('int')\n",
    "titanic['Embarked_C'] = (titanic['Embarked'] == 'C').astype('int')\n",
    "titanic = titanic.drop(['Sex', 'Embarked'], axis=1)\n",
    "\n",
    "# create X and y, test and train\n",
    "X_cols = titanic.columns\n",
    "X_cols = X_cols.drop('Survived')\n",
    "X_titanic = titanic[X_cols]\n",
    "y_titanic = titanic['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you have designed your `fit` method well, predict method will be naturally easy. If the node is a leaf, simply return the leaf value. If the node is not a leaf, call predict on one of its child nodes depending on whether it fits the condition.\n",
    "\n",
    "__If you need more help with these methods, ask a peer with programming experience, or you can seek help from the team.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_dt = dt.predict(X_test.values)\n",
    "sum(preds_dt == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have a decision tree, we can build a random forest, comprising of decision trees of randomised bootstraps of our dataset. At the simplest level, a random forest can be simply a list of decision trees that take a vote on the outcome of the prediction. This list can be an attribute of the random forest.\n",
    "\n",
    "The basic modification of random forests is the use of bootstrapping. Bootstrapping is done in a few lines of code through `np.random.choice`.\n",
    "\n",
    "Hence, to begin, build a simple random forest, that will initialise 5 trees through bootstrapping (sampling 100% with replacement), and predict the answer through a voting mechanism out of all the 5 trees. For computational efficiency, we recommend using `np.stack` and `np.array.mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_0 = RandomForest()\n",
    "rf_0.fit(X_train.values, y_train.values)\n",
    "preds_rf = rf_0.predict(X_test.values)\n",
    "sum(preds_rf == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we will implement `n_trees` to be tweakable. In addition, we will have a `subsample_size` parameter, which does the subsampling that the sklearn's random forest could not do. We can continue to use `np.random.choice`, but if subsample_size > 1, we can sample without replacement instead. (Or you could have another parameter to adjust that too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_1 = RandomForest(n_trees=10, subsample_size=0.8)\n",
    "rf_1.fit(X_train.values, y_train.values)\n",
    "preds_rf1 = rf_1.predict(X_test.values)\n",
    "sum(preds_rf1 == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we will implement the `feature_proportion` feature, which refers to the number of features we allow each tree to use. This further increases the randomness and eliminates overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_2 = RandomForest(n_trees=100, subsample_size=0.5, feature_proportion=0.5)\n",
    "rf_2.fit(X_train.values, y_train.values)\n",
    "preds_rf2 = rf_2.predict(X_test.values)\n",
    "sum(preds_rf2 == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may wish to attempt to implement other optional parameters of random forest. One important parameter is `max_features` which makes the tree lose some features at every node, or `max_depth`, which limits the number of levels the tree can have. However, we chose to leave these out, as they require tweaking at the decision tree level, which is an exercise left for your own choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Congratulations!__ You have finally come to the end of the week 1. Hope you had as much fun as we had building it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://www.ambitiouskitchen.com/wp-content/uploads/2014/03/glutenfreecookies-6.jpg\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
